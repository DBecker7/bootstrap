---
title: "The Bootstrap Method"
author: |
    | 
    | Dr. Devan Becker
    |
    | The University of Guelph
    | 
    | April 13, 2022
execute:
    cache: true
format: 
    revealjs:
        theme: 
            - serif
        slide-number: true
        css: reveal.css
        echo: false
---


# Invasion of Omicron

How quickly did omicron become dominant?

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 15})

omicron = pd.read_csv("data.csv")
plt.plot(omicron.date, omicron.prop, "o")
plt.xlabel("Date")
plt.ylabel("Proportion")
plt.title("Proportion of omicron in known variants")
plt.show()
```


# Logistic Growth Curve

![](fit-eq2c-1.png)

<sub><sub><sub>Source: [https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/](https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/)</sub></sub></sub>


# Fitting the Curve

Least squares, but no assumption of normality!

$$
(\widehat{scale}, \widehat{mid}) = \hat\theta = \text{argmin}_{\theta}(y_i - f(x_i | \theta))^2
$$

. . . 

```{python}
#| echo: true
#| eval: true
#| code-line-numbers: "5-8|3|3,10-14"
import numpy as np
import pandas as pd
from scipy.optimize import curve_fit
omicron = pd.read_csv("data.csv")

def logistic_growth(x, r, k): # assume asymptote is 1
    y = 1/(1 + np.exp((k - x) * r))
    return y

theta_hat, covs = curve_fit(f = logistic_growth, 
    xdata = omicron.date, 
    ydata = omicron.prop, 
    p0 = [0.3, 15])
```



# Challenge: Variance?

Point estimates are easy!

```{python}
#| echo: false

xseq = np.linspace(0, 30, 31)
yseq = logistic_growth(xseq, theta_hat[0], theta_hat[1])

plt.plot(omicron.date, omicron.prop, "o")
plt.plot(xseq, yseq, color = "black")
plt.xlabel("Date")
plt.ylabel("Proportion")
plt.title("Proportion of omicron in known variants")
plt.show()
```

# Recap

What is the standard error?

1. The sd of the distribution of sample means
2. The sd of the observed sample mean
3. The sd of a sample

. . . 

:::{.callout-note}
## Answer

Option 1 is the (simplified) definition.
:::


# Recap

Which is a reasonable estimate of the population mean?

1. The sample mean
2. The sample median
3. Halfway between $Q_1$ and $Q_3$
4. All have their merits and pitfalls

. . . 

:::{.callout-note}
## Answer

There's more than one way to estmate the centre!
:::



# Question of the Day

Which is a reasonable estimate of the population?

1. The sample mean
2. The sample mean and sample sd
3. The sample

. . . 

:::{.callout-note}
## Answer

If our sample is representative, then it can be used as an estimate!
:::



# Bootstrapping: Learning Outcomes


<br>

**If** the sample is a **plug-in estimate** of the population...

<br>

. . . 

then we can build the **sampling distribution** from nothing but the **sample**!



# Bootstrapping in Action

Re-fit with random indices (keeping $(x, y)$ pairs together).

```{python}
#| echo: true
#| code-line-numbers: "1,5|8-11|12|13-14"
n_boot = 5000
scale_tracker = np.zeros(n_boot)
midpoint_tracker = np.zeros(n_boot)

for i in range(n_boot):
    np.random.seed(i)

    indices = np.random.randint(
        low = 0, 
        high = omicron.shape[0], 
        size = omicron.shape[0]) # WITH REPLACEMENT
    omi = omicron.iloc[indices]
    theta, covs = curve_fit(logistic_growth, 
        omi.date, omi.prop, p0 = [0.3, 15])
    scale_tracker[i] = theta[0]
    midpoint_tracker[i] = theta[1]

```


# Results

```{python}
xseq = np.linspace(0, 30, 31)
yseq = logistic_growth(xseq, theta_hat[0], theta_hat[1])

figure, axis = plt.subplots(2, 2, dpi = 100)
figure.tight_layout(h_pad=2)

axis[0, 0].plot(omicron.date, omicron.prop, "o", 
    xseq, yseq, "-k")
axis[0, 0].set_title("Data with all resampled curves")

for i in range(midpoint_tracker.shape[0]):
    yi = logistic_growth(xseq, 
        scale_tracker[i], midpoint_tracker[i])
    axis[0, 0].plot(xseq, yi, color = "grey", alpha = 0.01, lw = 0.1)

#kr = np.polyfit(scale_tracker, midpoint_tracker, deg = 1)
#kseq = np.array([np.min(midpoint_tracker), np.max(midpoint_tracker)])
#rseq = np.array([np.min(scale_tracker), np.max(scale_tracker)])
#axis[0, 1].plot(scale_tracker, midpoint_tracker, "o")
#axis[0, 1].set_title("Scatterplot of scale versus midpoint")
#axis[0, 1].plot(rseq, kr[1] + kr[0]*rseq, "-r")

axis[0, 1].set_visible(False)

axis[1, 0].hist(scale_tracker, bins = 70)
axis[1, 0].set_title("Histogram of scale")
axis[1, 0].vlines(theta_hat[0], 0, 400, color = "red")

axis[1, 1].hist(midpoint_tracker, bins = 70)
axis[1, 1].set_title("Histogram of midpoint")
axis[1, 1].vlines(theta_hat[1], 0, 200, color = "red")
plt.show()
```


# Covariance between parameters

```{python}
from scipy.stats import kde
k = kde.gaussian_kde(np.stack([scale_tracker, midpoint_tracker]))
xi, yi = np.mgrid[scale_tracker.min():scale_tracker.max():100*1j, midpoint_tracker.min():midpoint_tracker.max():100*1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))

plt.pcolormesh(xi, yi, zi.reshape(xi.shape))
plt.xlabel("Scale")
plt.ylabel("Midpoint")
plt.show()
```


# Calculating a CI

::::{.columns}

::: {.column width="47%"}

1. Quantiles

0.025 and 0.975 $\phantom{t_\alpha}$

```{python}

rtiles = np.quantile(scale_tracker, [0.025, 0.975])

plt.figure(figsize = (4,3))

plt.hist(scale_tracker, bins = 70)
plt.vlines(rtiles, 0, 600, colors = "red")
plt.title("Sampling Distr. for scale")
plt.xlabel("Bootstrapped value of scale")
plt.ylabel("Count")
plt.text(rtiles[0], 200, "2.5%", 
    ha = "right", va = "center", size = 7,
    bbox = dict(boxstyle="larrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.text(rtiles[1], 200, "2.5%", 
    ha = "left", va = "center", size = 7,
    bbox = dict(boxstyle="rarrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.show()
```

Estimates sampling distr.

:::

::: {.column width="5%"}

:::

::: {.column width="47%"}
2. Normal (or $t$) CI

$\hat k \pm t_{\alpha/2} s_{k^*}$

```{python}
k_mean = np.mean(midpoint_tracker)
k_sd = np.sqrt(np.var(midpoint_tracker))

plt.figure(figsize = (4,3))
plt.hist(midpoint_tracker, bins = 70)
plt.vlines([k_mean, k_mean + k_sd], ymin = 0, ymax = 250, color = "red")
plt.text(k_mean + k_sd/2, 75, "1sd", 
    ha = "center", va = "center", size = 10,
    bbox = dict(boxstyle="rarrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.title("Sampling Distr. for midpoint")
plt.xlabel("Bootstrapped value of midpoint")
plt.ylabel("Count")
plt.show()
```

Estimates standard error.

:::
::::




# The Bootstrap Principle

**The sample is a plug-in estimate of the population.**

. . . 


:::{.callout-warning}
## The sample must be *representative* of the population.

Bootstrapping doesn't fix biased sampling or small sample sizes.
:::

. . . 

:::{.callout-warning}
## Centered near the mean of the sample not the mean of the population.

Bootstrapping does not improve the point estimate.
:::

. . . 

:::{.callout-warning}
## Correct assumptions about the population can be powerful.

Today's lesson has been on *non-parametric* bootstrapping.
:::

. . . 

:::{.callout-warning}
## Samples must be independent!

No time series models!
:::



# Discussion

<br>

Is the bootstrap good for medians?

# Try it Yourself!


1. Sample from a population.
2. Calculate the statistic
3. Calculate the bootstrap version of that statistic

Try it for:

:::{.callout-important icon=false}
## Quantiles

Median, 0.025, 0.975
:::

:::{.callout-tip icon=false}
## The $t$-statistic

Bootstrap is robust to deviations from normal populations!
:::

:::{.callout-warning icon=false}
## Mean of skewed distributions

What's your intuition? (This may be an assignment question...)
:::



# Summary

<br>

- Bootstrapping is simply **re-sampling** from your sample (**Plug-in Principle**)

<br>

- Bootstrapping works well for **complicated or unknown/unassumed sampling distributions**

<br>

- Bootstrap confidence intervals can be great, but **don't bootstrap blindly**!

# Next Steps

- Bias-corrected and accelerated bootstraps
- Parametric and smoothed bootstrap
- Permutation tests

<br>

**Further Reading:**

- Bootstrap for time series models


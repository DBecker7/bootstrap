---
title: "The Bootstrap Method"
author: |
    | 
    | Dr. Devan Becker
    |
    | The University of Western Ontario
    | 
    | March 15, 2022 ðŸŽ‚
execute:
    cache: true
format: 
    revealjs:
        theme: 
            - serif
        slide-number: true
        css: reveal.css
        echo: false
    pdf: 
        echo: true
---

# Recap

What is the standard error?

1. The sd of the distribution of sample means
2. The sd of the observed sample mean
3. The sd of a sample

. . . 

:::{.callout-note}
## Answer

Option 1.
:::

:::{.notes}
The concept we're going to cover today relates to sampling distributions.
So we'll start with a simple question: what is the standard error?

Since you're reading the pdf version, it's hard to stop you from reading the answer without thinking about it.
I still recommend thinking about it for a minute or two.

Okay, so the answer is 1: the standard deviation of the distribution of sample means.
By knowing the behaivour of sample means, we can say something about the population mean.
That is, we can do *inference*.
:::

# Recap

Which of the following is the standard error?

1. $\sigma/\sqrt{n}$
2. $s/\sqrt{n}$
3. $s/\sqrt{n-1}$
3. None of the above

. . .

:::{.callout-note}
## Answer

It all depends on *what assumptions we make*.
:::

:::{.notes}
The problem with the statement is the word "*the*". 
If we make different assumptions, we'll get a different definition for the standard error.
There is no single standard error, it's all based on context.
:::

# Recap

Which is a reasonable estimate of the population mean?

1. The sample mean
2. The sample median
3. Halfway between $Q_1$ and $Q_3$
4. All have their merits

. . . 

:::{.callout-note}
## Answer

There's more than one way to estmate the centre!
:::

:::{.notes}
Clearly, the correct answer is "All have their merits".
The point of this question is to prime you for the next:
:::

# Question of the Day

Which is a reasonable estimate of the population?

1. The sample mean
2. The sample mean and sample sd
3. The sample

:::{.notes}
Todays lesson focuses on this idea: the sample can be seen as an estimate of the population, and can be used to construct a sampling distribution!
:::



:::{.notes}
With your mind primed for thinking about sampling distributions, let's get into the lesson.
A colleague recently reached out to me to calculate a confidence interval for a logistic growth model.
For this application, we're looking at the proportion of cases of COVID-19 that were known to be Omicron.
Note that the denominator for this is all cases with known variants - we're not considering cases where we didn't know the variant.
As with many infectious diseases, Omicron was seen in very low numbers for a long time before it eventually reached critical mass and took over the population.
Once it had made up the majority of cases, it's growth slowed as it took the last few cases.
The plot above shows weekly data, but my research has daily information which is much more variable.
:::

# Invasion of Omicron

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 15})

omicron = pd.read_csv("data.csv")
plt.plot(omicron.date, omicron.prop, "o")
plt.xlabel("Date")
plt.ylabel("Proportion")
plt.show()
```

:::{.notes}
This plot is closer to the data I actually have.
I have the proportion of the variant for most days, and so I simulated some data that has similar features to mine.
:::

# Logistic Growth Curve

![](fit-eq2c-1.png)

<sub><sub><sub>Source: [https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/](https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/)</sub></sub></sub>

:::{.notes}
In this context, a logistic growth model is just the assumed shape of the curve.
Just like how the normal distribution is completely characterized by its mean and standard deviation, the growth curve is characterized by the following three values:

- The asymptote, which is the maximum value that the curve will reach. In our case, we know that this is 1.
- The midpoint, which is the point at which the curve reaches half of the asymptote.
- The scale, which can be seen as the slope of the curve at the midpoint.

We don't need to go into too much detail for the model, other than to say that it is our assumption about the shape of the relationship and we haven't made any distributional assumptions.
:::

# Fitting the Curve

Least squares, but no assumption of normality!

$$
(\widehat{scale}, \widehat{mid}) = \hat\theta = \text{argmin}_{\theta}(y_i - f(x_i | \theta))^2
$$


```{python}
#| echo: true
#| eval: true
#| code-line-numbers: "3|3,10-14"
import numpy as np
import pandas as pd
from scipy.optimize import curve_fit
omicron = pd.read_csv("data.csv")

def logistic_growth(x, r, k): # assume asymptote is 1
    y = 1/(1 + np.exp((k - x) * r))
    return y

theta_hat, covs = curve_fit(f = logistic_growth, 
    xdata = omicron.date, 
    ydata = omicron.prop, 
    p0 = [0.3, 15])
```

:::{.notes}
Since we're not making any distributional assumptions, we cannot use maximum likelihood.
We can still find the least squares between the line and the points, but we don't have the convenient calculus behind linear models.
Instead, it's an optimization routine.
Note that $asym$ is known to be one, so we only need to consider the slope and the scale.

Note that we can't even say that the observations are just random error around the line - this assumes additivity, which is not one of our assumptions!
There's a theme here: we've assumed the shape of the relationship and absolutely nothing else.
:::

# Bootstrapping

Re-fit with random indices (keep x and y together).

```{python}
#| echo: true
#| code-line-numbers: "1,5|8-11|12|13-14"
n_boot = 5000
scale_tracker = np.zeros(n_boot)
midpoint_tracker = np.zeros(n_boot)

for i in range(n_boot):
    np.random.seed(i)

    indices = np.random.randint(
        low = 0, 
        high = omicron.shape[0], 
        size = omicron.shape[0]) # WITH REPLACEMENT
    omi = omicron.iloc[indices]
    theta, covs = curve_fit(logistic_growth, 
        omi.date, omi.prop, p0 = [0.3, 15])
    scale_tracker[i] = theta[0]
    midpoint_tracker[i] = theta[1]

```

:::{.notes}
A (non-parametric) bootstrap sample is just a random sample from the sample that you already have.
In practice, this means sampling $n$ integers from 1 to $n$, and then taking these indices from your dataset.
Of course, this means that we're sampling with replacement - we will almost always have repeated values in our sample.

Just like with samples from the population, each time we do this we'll have a different mean value.
Just like with samples from the population, the standard deviation of these means is an estimate for the standard error of the mean.

Aside: We are making some assumptions about the population, though.
The bootstrap doesn't work when the population has infinite variance or when there are, say, outliers in the population that don't exist in our sample.
:::

# Results

```{python}
xseq = np.linspace(0, 30, 31)
yseq = logistic_growth(xseq, theta_hat[0], theta_hat[1])

figure, axis = plt.subplots(2, 2, dpi = 100)
figure.tight_layout(h_pad=2)

axis[0, 0].plot(omicron.date, omicron.prop, "o", 
    xseq, yseq, "-k")
axis[0, 0].set_title("Data with all resampled curves")

for i in range(midpoint_tracker.shape[0]):
    yi = logistic_growth(xseq, 
        scale_tracker[i], midpoint_tracker[i])
    axis[0, 0].plot(xseq, yi, color = "grey", alpha = 0.01, lw = 0.1)

#kr = np.polyfit(scale_tracker, midpoint_tracker, deg = 1)
#kseq = np.array([np.min(midpoint_tracker), np.max(midpoint_tracker)])
#rseq = np.array([np.min(scale_tracker), np.max(scale_tracker)])
#axis[0, 1].plot(scale_tracker, midpoint_tracker, "o")
#axis[0, 1].set_title("Scatterplot of scale versus midpoint")
#axis[0, 1].plot(rseq, kr[1] + kr[0]*rseq, "-r")

axis[0, 1].set_visible(False)

axis[1, 0].hist(scale_tracker, bins = 70)
axis[1, 0].set_title("Histogram of scale")
axis[1, 0].vlines(theta_hat[0], 0, 400, color = "red")

axis[1, 1].hist(midpoint_tracker, bins = 70)
axis[1, 1].set_title("Histogram of midpoint")
axis[1, 1].vlines(theta_hat[1], 0, 200, color = "red")
plt.show()
```

:::{.notes}
Here are the results of the bootstrap sampling.

The first plot shows all of curves that were estimated by resamples as thin grey lines.
Since there are 5,000 of them, they appear as one single line.
Notice how the line fit to the original data isn't straight through the middle of this cloud of lines.
The two outliers around x = 8 seem to have a large effect on our parameter estimates!
Note that we don't have a rigorous definition of influence in this model, but the bootstrap has revealed this to us!

The second plot shows the covariance between the two parameters, with the red line indicating a linear model fit.
Obviously, when we change the slope we also must change the midpoint to make the line still fit the data.
What's nice is that the bootstrap method has no problem picking up on this, and we didn't need to make any parametric assumptions!

The third and fourth plots show the estimated sampling distributions along with a red line indicating the estimate from the original sample.
The scale clearly has a non-normal, skewed distribution for this sample, whereas the midpoint does seem somewhat normal looking.
:::

# Covariance between parameters

```{python}
from scipy.stats import kde
k = kde.gaussian_kde(np.stack([scale_tracker, midpoint_tracker]))
xi, yi = np.mgrid[scale_tracker.min():scale_tracker.max():100*1j, midpoint_tracker.min():midpoint_tracker.max():100*1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))

plt.pcolormesh(xi, yi, zi.reshape(xi.shape))
plt.xlabel("Scale")
plt.ylabel("Midpoint")
plt.show()
```

:::{.notes}
As we saw in the previous slide, there is covariance between the parameters.
This KDE estimate shows that this is not a linear correlation: there is a slight curve to this relationship, and the most common bootstrap estimates are not in the center of the distribution.
:::

# Why does it work? 

Consider these questions first:

- Why is the sample mean close to the population mean?

. . . 

:::{.callout-tip}
## Sampling design

A good sample is *representative of the population*.
:::

. . . 

- Why do we use $\hat p(1 - \hat p)/n$ in the binomial CI?

. . . 

:::{.callout-tip}
## The Plug-In Principle

An estimate can be used in place of an unknown quantity.
:::

:::{.notes}
It almost seems magical that we can estimate the sampling distribution without making assumptions about the population distribution.
So why does this work?

Let's think back to the sample mean.
We intuitively know that it's a decent estimate of the population mean/
Why is that?
It's because we assume that the sample is *representative* of the population.

Now think about confidence intervals for proportions. 
When we need an estimate for the standard error, we don't have a separate estimate for the standard error.
Instead, we simply *plug in* the estimate of the proportion.

This is the plug-in principle: If we have a good estimate, we can use it in place of an unknown quantity.
For example, if we know that the mean of one population should be twice as large as the mean of another population, we can plug-in the mean estimate for the first to estimate the second.
:::


# Bootstrap Plug-In

**The sample is a plug-in estimate of the population.**

. . . 

<br>

Some things to note

- Sample must be *representative* of population
- Mean of bootstrap is (close to) mean of *sample*, not themena of the population
- Correct assumptions about population can be powerful

:::{.notes}
For bootstrap, we're using the sample as a plug-in estimate of the population.
This heavily relies on the assumption that we have a sample that is representative of the population.
Assuming we have this assumption, we don't need to make assumptions about the parametric form of the population!

It is important to note that the bootstrap doesn't improve on our current estimate.
For the sample mean, the bootstrap distribution is centered on the sample mean, *NOT* the population mean.
This is why it's useful for confidence intervals, but don't for a second think that it will get us more information about the population mean.
:::

# Calculating a CI

::::{.columns}

::: {.column width="47%"}

1. Quantiles

0.025 and 0.975 $\phantom{t_\alpha}$

```{python}

rtiles = np.quantile(scale_tracker, [0.025, 0.975])

plt.figure(figsize = (4,3))

plt.hist(scale_tracker, bins = 70)
plt.vlines(rtiles, 0, 600, colors = "red")
plt.title("Sampling Distr. for scale")
plt.xlabel("Bootstrapped value of scale")
plt.ylabel("Count")
plt.text(rtiles[0], 200, "2.5%", 
    ha = "right", va = "center", size = 7,
    bbox = dict(boxstyle="larrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.text(rtiles[1], 200, "2.5%", 
    ha = "left", va = "center", size = 7,
    bbox = dict(boxstyle="rarrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.show()
```

Estimates sampling distr.

:::

::: {.column width="5%"}

:::

::: {.column width="47%"}
2. Normal (or $t$) CI

$\hat k \pm t_{\alpha/2} s_{k^*}$

```{python}
k_mean = np.mean(midpoint_tracker)
k_sd = np.sqrt(np.var(midpoint_tracker))

plt.figure(figsize = (4,3))
plt.hist(midpoint_tracker, bins = 70)
plt.vlines([k_mean, k_mean + k_sd], ymin = 0, ymax = 250, color = "red")
plt.text(k_mean + k_sd/2, 75, "1sd", 
    ha = "center", va = "center", size = 10,
    bbox = dict(boxstyle="rarrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.title("Sampling Distr. for midpoint")
plt.xlabel("Bootstrapped value of midpoint")
plt.ylabel("Count")
plt.show()
```

Estimates standard error.

:::
::::

:::{.notes}
Now that we have a way of estimating the sampling distribution, what do we do with it?
There are two main approaches.

First, we can just use the bootstrap sampling distribution as a plug-in estimate for the true sampling distribution.
This means looking at all bootstrap samples and finding, say, the 2.5 and 97.5 percentiles.

Second, we can use the standard deviation from the sampling distributions as the standard error and calculate our usual confidence interval as the sample mean plus and minus the standard error.
If the bootstrap sampling distribution looks skewed we can make appropriate adjustments.

There are other approaches out there, such as calculating the average width of parametric (normal) confidence intervals built from each re-sample or incorporating bias, but these can be explored in assignments or in your spare time.
:::


# Try it Yourself!

:::{.callout-important icon=false}
## The Median (and other Quantiles) 

Bootstrap emulates population, but median uses 1 or 2 points. 
:::

:::{.callout-important icon=false}
## Small samples

Bootstrap requires at least moderate-sized samples.
:::

:::{.callout-tip icon=false}
## The t-statistic

Bootstrap estimates the joint distribution of mean and sd, and is more robust to deviations from normality!
:::

:::{.callout-warning icon=false}
## Skewed Distributions?

What's your intuition? (This may be an assignment question...)
:::

See the code in the repo!

:::{.notes}
The bootstrap is not a magic bullet that works for everything.
For the median, there is code in the repo to estimate the sampling distribution from a known (non-normal) population.
You'll need to take a sample, the calculate a bootstrap confidence interval for that sample.

Alternatively, we can look at confidence intervals for the mean, but when the population is heavily skewed.

One of these will be part of a future assessment!
:::

# Summary

- Bootstrapping is simply **re-sampling** from your sample
    - **Plug-in Principle** for sampling distribuions


- Bootstrapping works well for **complicated or unknown/unassumed sampling distributions**
    - Covariance between parameters


- Bootstrap confidence intervals can be great!
    - ... but **don't bootstrap blindly**!

# Next Steps

**Next Lessons:**

- Bias-corrected and accelerated bootstraps
- Parametric and smoothed bootstrap
- Permutation tests

<br>

**Further Reading:**

- Bootstrap for time series models

:::{.notes}
In the next lesson, we'll look at the bootstrap method for hypothesis testing, in particular how the bootstrap t-statistic can be *better* because it incorporates the covariance between the mean and the standard deviation.
However, it is also too small by a factor of $\sqrt{(n-1)/n}$.

In the lesson after that, we'll look at two extensions of bootstrapping, including when we assume something about the shape of the population and a hybrid between no assumptions and parametric assumptions.
:::

---
title: "The Bootstrap Method"
author: "Devan Becker"
format: 
  revealjs:
    theme: serif
    slide-number: true
    css: reveal.css
    echo: false
  pdf: 
    echo: true
---

# Recap

What is the standard error?

1. The sd of the distribution of sample means
2. The sd of the observed sample mean
3. The sd of a sample

:::{.notes}
The concept we're going to cover today relates to sampling distributions.
So we'll start with a simple question: what is the standard error?

Since you're reading the pdf version, it's hard to stop you from reading the answer without thinking about it.
I still recommend thinking about it for a minute or two.

Okay, so the answer is 1: the standard deviation of the distribution of sample means.
By knowing the behaivour of sample means, we can say something about the population mean.
That is, we can do *inference*.
:::

# Recap

Which of the following is the standard error?

1. $\sigma/\sqrt{n}$
2. $s/\sqrt{n}$
3. $s/\sqrt{n-1}$
3. None of the above

. . .

:::{.callout-note}
## Answer

It all depends on *what assumptions we make*.
:::

:::{.notes}
The problem with the statement is the word "*the*". 
If we make different assumptions, we'll get a different definition for the standard error.
There is no single standard error, it's all based on context.
:::

# Recap

Which is a reasonable estimate of the population mean?

1. The sample mean
2. The sample median
3. Halfway between $Q_1$ and $Q_3$
4. All have their merits

:::{.notes}
Clearly, the correct answer is "All have their merits".
The point of this question is to prime you for the next:
:::

# Question of the Day

Which is a reasonable estimate of the population?

1. The sample mean
2. The sample mean and sample sd
3. The sample

:::{.notes}
Todays lesson focuses on this idea: the sample can be seen as an estimate of the population, and can be used to construct a sampling distribution!
:::


# Invasion of Omicron

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Only need to run once to download the data
#voc = pd.read_csv("https://health-infobase.canada.ca/src/data/covidLive/covid19-epiSummary-variants.csv")
#voc.columns = ["Grouping", "Identifier", "Lineage", "Proportion", "Collection_Week"]
#voc.to_csv("voc.csv")
voc = pd.read_csv("voc.csv")

any_omicron = voc[voc.Identifier == "Omicron"]
any_omicron = any_omicron.loc[any_omicron.Collection_Week >= "2021-10"]
any_omicron['Collection_Week'] = pd.to_datetime(any_omicron['Collection_Week'], infer_datetime_format=True)
all_omicron = any_omicron.groupby("Collection_Week", as_index = False).Proportion.sum()

all_omicron.plot("Collection_Week", "Proportion", kind = "scatter")
plt.ylabel("Proportion of Cases of Omicron")
plt.xlabel("Collection Week")
plt.show()
```

Source: [https://health-infobase.canada.ca/covid-19/epidemiological-summary-covid-19-cases.html](https://health-infobase.canada.ca/covid-19/epidemiological-summary-covid-19-cases.html)

:::{.notes}
With your mind primed for thinking about sampling distributions, let's get into the lesson.
A colleague recently reached out to me to calculate a confidence interval for a logistic growth model.
For this application, we're looking at the proportion of cases of COVID-19 that were known to be Omicron.
Note that the denominator for this is all cases with known variants - we're not considering cases where we didn't know the variant.
As with many infectious diseases, Omicron was seen in very low numbers for a long time before it eventually reached critical mass and took over the population.
Once it had made up the majority of cases, it's growth slowed as it took the last few cases.
The plot above shows weekly data, but my research has daily information which is much more variable.
:::

# Logistic Growth Curve

![](fit-eq2c-1.png)

Source: [https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/](https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/)

:::{.notes}
In this context, a logistic growth model is just the assumed shape of the curve.
Just like how the normal distribution is completely characterized by its mean and standard deviation, the growth curve is characterized by the following three values:

- The asymptote, which is the maximum value that the curve will reach. In our case, we know that this is 1.
- The midpoint, which is the point at which the curve reaches half of the asymptote.
- The scale, which can be seen as the slope of the curve at the midpoint.

We don't need to go into too much detail for the model, other than to say that it is our assumption about the shape of the relationship and we haven't made any distributional assumptions.
:::

# Fitting the Curve

Least squares, but no assumption of normality!

$$
\text{argmin}_{asym, mid, scale}(y_i - f(x_i, asym = 1, mid, scale))^2
$$

- Constrained to $[0,1]$.
- Error structure may not even be additive! 

:::{.notes}
Since we're not making any distributional assumptions, we cannot use maximum likelihood.
We can still find the least squares between the line and the points, but we don't have the convenient calculus behind linear models.
Instead, it's an optimization routine.
Note that $asym$ is known to be one, so we keep that in this 

We can't even say that the observations are just random error around the line - this assumes additivity, which is not one of our assumptions!
There's a theme here: we've assumed the shape of the relationship and absolutely nothing else.
:::

# Activity

**Grow the Sampling Distribution**

Sampling from the population is difficult, so I upload some samples to GitHub: [Colab Notebook](https://colab.research.google.com/drive/1dIg3mql2w6KCEstAo9H2FK9ak-qIyPZx) (Google account required).

<br>

Put your sample estimates of $(scale, midpoint)$ in the chat!

:::{.notes}
In class, I told you that the samples drawn in the linked notebook were obtained from a very complicated distribution and that's why you couldn't run it on your own computer.
That was a lie.
Instead, they were actually bootstrap samples!

A (non-parametric) bootstrap sample is just a random sample from the sample that you already have.
In practice, this means sampling $n$ integers from 1 to $n$, and then taking these indices from your dataset.
Of course, this means that we're sampling with replacement - we will almost always have repeated values in our sample.

Just like with samples from the population, each time we do this we'll have a different mean value.
Just like with samples from the population, the standard deviation of these means is an estimate for the standard error of the mean.

Go ahead and run the example code a bunch of times. Notice how the sampling distribution of the scale parameter is clearly non-normal, and that there's clearly correlation between the two parameters that a univariate sampling distribution would not capture.
Also notice how we can use the sampling distributions to calculate confidence intervals, *even though we never made distributional assumptions*. 

Aside: We are making some assumptions about the population, though.
The bootstrap doesn't work when the population has infinite variance or when there are, say, outliers in the population that don't exist in our sample.
:::


# I Lied to You

Those are actually **bootstrap samples!**

```{python}
#| echo: true
#| eval: false
#| code-line-numbers: "8-12"
import pandas as pd
import numpy as np
xy = pd.read_csv("data.csv")

for i in range(1000):
    np.random.seed(i)

    indices = np.random.randint(
        low = 0, 
        high = xy.shape[0], 
        size = xy.shape[0])
    xyi = xy.iloc[indices]

    xyi.sort_values("x")
    xyi.to_csv("samples/sample_" + str(i).zfill(4) + ".csv", 
        index = False)

```


# Why does it work? 

Consider these questions first:

- Why is the sample mean close to the population mean?

. . . 

:::{.callout-tip}
## Sampling design

A good sample is *representative of the population*.
:::

. . . 

- Why do we use $\hat p(1 - \hat p)/n$ in the binomial CI?

. . . 

:::{.callout-tip}
## The Plug-In Principle

An estimate can be used in place of an unknown quantity.
:::

:::{.notes}
It almost seems magical that we can estimate the sampling distribution without making assumptions about the population distribution.
So why does this work?

Let's think back to the sample mean.
We intuitively know that it's a decent estimate of the population mean/
Why is that?
It's because we assume that the sample is *representative* of the population.

Now think about confidence intervals for proportions. 
When we need an estimate for the standard error, we don't have a separate estimate for the standard error.
Instead, we simply *plug in* the estimate of the proportion.

This is the plug-in principle: If we have a good estimate, we can use it in place of an unknown quantity.
For example, if we know that the mean of one population should be twice as large as the mean of another population, we can plug-in the mean estimate for the first to estimate the second.
:::


# Bootstrap Plug-In

**The sample is a plug-in estimate of the population.**

. . . 

<br>

Some things to note

- Sample must be *representative*
- Mean of bootstrap is (close to) mean of *sample*
- Correct assumptions are powerful

:::{.notes}
For bootstrap, we're using the sample as a plug-in estimate of the population.
This heavily relies on the assumption that we have a sample that is representative of the population.
Assuming we have this assumption, we don't need to make assumptions about the parametric form of the population!

It is important to note that the bootstrap doesn't improve on our current estimate.
For the sample mean, the bootstrap distribution is centered on the sample mean, *NOT* the population mean.
This is why it's useful for confidence intervals, but don't for a second think that it will get us more information about the population mean.
:::

# Calculating the CI

::::{.columns}

::: {.column width="40%"}

1. Quantiles

0.025 and 0.975

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

def logistic_growth(x, r, k):
    y = 1/(1 + np.exp((k - x) * r))
    return y

xy = pd.read_csv("data.csv")

n_boot = 5000
r_tracker = np.zeros(n_boot)
k_tracker = np.zeros(n_boot)

for i in range(n_boot):
    np.random.seed(i)

    indices = np.random.randint(
        low = 0, 
        high = xy.shape[0], 
        size = xy.shape[0])
    xyi = xy.iloc[indices]
    popt, pcov = curve_fit(logistic_growth, 
        xyi.x.to_numpy(), xyi.y.to_numpy(), 
        p0 = [0.3, 15])
    r_tracker[i] = popt[0]
    k_tracker[i] = popt[1]

rtiles = np.quantile(r_tracker, [0.025, 0.975])

plt.figure(figsize = (4,3))

plt.hist(r_tracker, bins = 50)
plt.vlines(rtiles, 0, 600, colors = "red")
plt.title("Sampling Distr. for scale")
plt.xlabel("Bootstrapped value of scale")
plt.ylabel("Count")
plt.text(rtiles[0], 200, "2.5%", 
    ha = "right", va = "center", size = 7,
    bbox = dict(boxstyle="larrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.text(rtiles[1], 200, "2.5%", 
    ha = "left", va = "center", size = 7,
    bbox = dict(boxstyle="rarrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.show()
```

:::

::: {.column width="40%"}
2. Standard error

$\hat k \pm t_{\alpha/2} s_{k^*}$

```{python}
#| fig-height: 3in
k_mean = np.mean(k_tracker)
k_sd = np.sqrt(np.var(k_tracker))

plt.figure(figsize = (4,3))
plt.hist(k_tracker, bins = 60)
plt.vlines([k_mean, k_mean + k_sd], ymin = 0, ymax = 250, color = "red")
plt.text(k_mean + k_sd/2, 75, "1sd", 
    ha = "center", va = "center", size = 10,
    bbox = dict(boxstyle="rarrow,pad=0.3", fc = "white", 
        ec = "b", lw = 2))
plt.title("Sampling Distr. for midpoint")
plt.xlabel("Bootstrapped value of midpoint")
plt.ylabel("Count")
plt.show()
```

:::
::::

:::{.notes}
Now that we have a way of estimating the sampling distribution, what do we do with it?
There are two main approaches.

First, we can just use the bootstrap sampling distribution as a plug-in estimate for the true sampling distribution.
This means looking at all bootstrap samples and finding, say, the 2.5 and 97.5 percentiles.

Second, we can use the standard deviation from the sampling distributions as the standard error and calculate our usual confidence interval as the sample mean plus and minus the standard error.
If the bootstrap sampling distribution looks skewed we can make appropriate adjustments.

There are other approaches out there, such as calculating the average width of parametric (normal) confidence intervals built from each re-sample or incorporating bias, but these can be explored in assignments or in your spare time.
:::

# Homework

Show that the bootstrap method:

- Is not great for the median (and other quantiles).
  - What does this mean for quantiles method?

- Is (good/bad?) for skewed distributions (e.g. exponential).

See the code in the repo!

:::{.notes}
The bootstrap is not a magic bullet that works for everything.
For the median, there is code in the repo to estimate the sampling distribution from a known (non-normal) population.
You'll need to take a sample, the calculate a bootstrap confidence interval for that sample.

Alternatively, we can look at confidence intervals for the mean, but when the population is heavily skewed.

One of these will be part of a future assessment!
:::

# Next Lesson

- Bootstrap t statistics
  - Bootstrap is sometimes *better*?!?

- Bias-corrected and accelerated bootstraps

- Parametric and smoothed bootstrap

:::{.notes}
In the next lesson, we'll look at the bootstrap method for hypothesis testing, in particular how the bootstrap t-statistic can be *better* because it incorporates the covariance between the mean and the standard deviation.
However, it is also too small by a factor of $\sqrt{(n-1)/n}$.

In the lesson after that, we'll look at two extensions of bootstrapping, including when we assume something about the shape of the population and a hybrid between no assumptions and parametric assumptions.
:::
